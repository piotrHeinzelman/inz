
Komentarze do pracy
2 Wydajność - wydajność czego?
2.1 Nie rozumiem po co wspominać o tym i to zaledwie w 2 zdaniach? (Maszyna Turinga)
2.2 Czy chodzi wyłącznie o translację punktów czy raczej o coś więcej (macierze na to wskazują). Nie wiem jak rozpisany wzór ma się do tych macierzy. 
Ogólnie rozdział 2 jest niezrozumiały. Wydaje mi się, że dostrzegam ideę, czyli pokazanie sposobów realizacji podstawowych obliczeń dodawania i mnożenia (których to operacji w wielu dziedzinach trzeba wykonywać miliardy czy biliony na sekundę), ale to co jest napisane jest niezrozumiałe, powyrywane z kontekstu, fragmentaryczne. Raczej trzeba wyjść od zdefiniowania tej potrzeby i potem pokazać, jak to w kolejnych etapach rozwoju komputerów robiono. ()
 
3 (Model perceptronu warstwowego MLP)
Nie można zaczynać rozdziału od rysunku.
Perceptron jest WIELOwarstwowy
W ogóle nie wiadomo po co o tym jest napisane. To jakoś trzeba uzasadnić.
 
4 (Modele sieci splotowych CNN)
Też nie wiadomo dlaczego nagle o tego rodzaju sieci Pan wspomina. We wszystkich tych rozdziałach brakuje jakiegoś wprowadzenia, które uzasadniłoby pisanie o tym. Plus te 3 rozdziały należy wrzucić do jednego, bo to raptem 11 stron jest, w tym sporo rysunków.
 
5.1
Kod w Javie opisany jako Python. Brakuje kodu własnej implementacji w Pythonie i Matlabie, nie wiadomo co to JavaBigDecimal. na wykresie jest C, a nie C++. Choć ten kod to w sumie C, a nie C++. DLa C++ powszechnie używana jest biblioteka Eigen, w której jest m.in. regresja liniowa. Skoro sprawdza Pan funkcję "wbudowaną" dla Pythona, to wypadałoby sprawdzić analogiczną dla C++.
 
5.2
Znowu niejasności i nie wiadomo dlaczego tak, a nie inaczej, oraz jak. Są 2 fragmenty kodu, ale sprawdzanych wersji kodu było znacznie więcej. Nie wiadomo więc co i jak tam było zrobione. W szczególności nie wiadomo jak uruchomił Pan sci-kit na GPU. Normalnie nie bardzo się da chyba.
Ogólnie eksperyment pokazuje, że nie warto angażować GPU do relatywnie prostych sieci i małych zbiorów danych. 
 
5.3, 5.4 Ogólnie pokazują, że dla większych sieci GPU się opłaca
5.5 nie rozumiem podpisu C++ Torch? Yolo8 w pythonie działa tak samo szybko na CPU i GPU? Coś tu nie gra. Sprawdziłem i jest kilkukrotna (zależy od sieci)
 
 
Na razie praca, w bardzo chaotyczny i niejasny sposób, pokazuje (i to nie zawsze), że GPU opłaca się dla większych sieci, a dla mniejszych nie. Zasadnicze pytanie: w jakim języku najlepiej napisać kod dla głębokich sieci neuronowych pozostaje bez odpowiedzi. Za mało jest tu eksperymentów i są one praktycznie nieopisane. O rozpoznawaniu twarzy są 2 zdania! A powinno być 10 stron. W założeniach miało to być zrobione w Matlabie, w Pythonie i jeszcze w czymś.  W zakresie ZABRAKŁO wyszczególnienia pytorcha, jako że torch i tensorflow to dwie podstawowe biblioteki do głębokiego uczenia. Tu brakuje też rozróżnienia czasu uczenia i czasu wnioskowania. Podejrzany jest też fakt, że Matlab jest szybszy. No ale kompletnie nie wiadomo co tam było robione i jak. I tak wygląda cała praca... To MUSI zostać skonkretyzowane. Trzeba pokazać cały proces w Matlabie, w Pythonie (najlepiej z tensorflow i z pytorchem) oraz ewentualnie jeszcze w torchc++ (o ile da się to łatwo uruchomić). Czyli potrzebny jest kod, przebieg uczenia, czas uczenia, czas wnioskowania, porównanie dokładności sieci, potem ocena łatwości przeprowadzenia całego procesu - w jakim środowisku jest to najprostsze, najprzyjaźniejsze, w jakim mamy większą kontrolę nad tym co robimy. I tak dla kilku zadań wymienionych w zakresie pracy. Wówczas będzie można porównać te języki
 