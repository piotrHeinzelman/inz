\begin{easyappendix}{Uogólniona reguła delty}
Rozważmy sieć jednowarstwową z elementami przetwarzającymi o nieliniowej, lecz niemalejącej i różniczkowalnej funkcji aktywacji F wówczas zmianę wag przy prezentacji \(\mu\)-tego wzorca  można opisać równaniem:
\begin{equation}
       \Delta w_{ji} = -\eta \frac {\partial \xi}{\partial w_{ji}} = - \eta \frac{\partial \xi}{\partial y_j} \frac{\partial y_j}{\partial w_{ji}} =
       - \eta \frac{\partial \xi}{\partial z_j} 
       \frac{\partial z_j}{\partial y_{j}}
       \frac{\partial y_j}{\partial w_{ji}},
\end{equation}

przy czym:
\begin{equation}
\frac{\partial \xi}{\partial z_j} = (s-z_j), \text{ z def. } ((\frac{1}{2}(x-y)^2 )' = (x-y),
\end{equation}

\begin{equation}
        \frac{\partial z_j}{\partial y_{j}} = F'(y_j),
\end{equation}

\begin{equation}
       \frac{\partial y_j}{\partial w_{ji}} = x_i;
\end{equation}

stąd ostatecznie wzór przyjmuje postać:
\begin{equation}
\Delta w_{ji}= \eta F'(y)(S-z_j)x_i = \eta F'(y)(S-F(y))x_i = \eta F'(y)(s_1, s_2  \dots s_m -F(x_1*w_1,x_2*w_2  \dots ))x_i
\end{equation}
oraz dla warstwy ukrytej:
\begin{equation}
\Delta w_{ji}= \eta F'(y_j) \sum_{i=1}^{n_{m+1}} {F'(y_j)(s^{^{m+1}}-z^{^{m+1}}_j)w_{ji}}
\end{equation}

\end{easyappendix}






\begin{easyappendix}{Algorytm propagacji wstecznej}

Algorytm ten \cite{korbicz1994}, podaje on przepis na zmianę wag \(w_{ij}\) dowolnych połączeń elementów przetwarzających rozmieszczonych w sąsiednich warstwach sieci jednokierunkowej. Jest on oparty na minimalizacji sumy kwadratów błędów uczenia z wykorzystaniem optymalizacyjnej metody największego spadku \cite{wit1986}. Dzięki zastosowaniu specyficznego sposobu propagowania błędów uczenia sieci powstałych na wyjściu, tzn. przesyłania ich do warstwy wyjściowej od wejściowej, algorytm propagacji wstecznej stał się jednym z najskuteczniejszych algorytmów uczenia sieci. 
Rozważamy sieć jednowarstwową o liniowych elementach przetwarzających. Załóżmy, że mamy \(P\)-elementowy zbiór wzorców. Przy prezentacji \(\mu\)-tego wzorca możemy zdefiniować błąd: 
\begin{equation}
       \delta^\mu_j =s^\mu_j-z^\mu_j=s^\mu-y^\mu_j=s^\mu-\sum_{i=0}^{m} w_{ij}x^\mu_i,
\end{equation}
gdzie \(s_j^\mu\), \(y^\mu_j\) oznaczają odpowiednio oczekiwane i aktualne wartości wyjścia \(j\)-tego elementu oraz ważoną sumę wejść wyznaczoną w jego sumatorze przy prezentacji \(\mu\)-tego wzorca. \(x^\mu_i\) i-ta składowa \(\mu\)-tego wektora wejściowego, \(w_{ji}\) - oznacza wagę połączenia pomiędzy \(j\)-tym elementem warstwy wyjściowej a \(i\)-tym elementem warstwy wejściowej. \(m\)-liczba wejść.
\newline
Jako miarę błędu sieci \(\xi\) wprowadzimy sumę po wszystkich wzorcach błędów powstałych przy prezentacji każdego z nich:
\begin{equation}
       \xi = \sum_{\mu=0}^{P}\xi_\mu = \frac {1}{2} \sum ^{P}_{\mu=1} \sum ^{n}_{j=1} (s^\mu-y^\mu)^2,
\end{equation}
gdzie
\begin{equation}
       \xi_\mu = \frac {1}{2} \sum ^{n}_{j=1} (s^\mu-y^\mu)^2,
\end{equation}
\textbf{Problem uczenia sieci to zagadnienie minimalizacji funkcji błędu \(\xi\).} Jedną z najprostszych metod minimalizacji jest gradientowa metoda największego spadku \cite{wit1986}. Jest to metoda iteracyjna, która poszukuje kolejnego lepszego punktu w kierunku przeciwnym do gradientu funkcji celu w danym punkcie. Stosując powyższą metodę do uczenia sieci, zmiana \(\Delta w_{ji}\)  wagi połączenia winna spełniać relację:

\begin{equation}
       \Delta w_{ji} = -\eta \frac{\partial\xi}{\partial w_{ji}} = -\eta \sum ^{P}_{\mu=1} \frac {\partial \xi_\mu}{\partial w_{ji}} = -\eta \sum ^{P}_{\mu=1} \frac {\partial \xi_\mu}{\partial z^\mu_j} \frac{\partial z_j^\mu}{\partial w_{ji}}
\end{equation}
gdzie \(\eta\) oznacza współczynnik proporcjonalności.
W przypadku elementów liniowych mamy:
\begin{equation}
       \frac{\partial\xi_\mu}{\partial z^\mu_j} = -(s^\mu_j-z^\mu_j) = -\delta^\mu_j, 
\end{equation}
\begin{equation}
       \frac{\partial z_j^\mu}{\partial w_{ji}} = \frac{\partial y^\mu_j}{\partial w_{ji} } = x _i^\mu
\end{equation}
stąd otrzymujemy:
\begin{equation}
       \Delta w_{ji} = \eta \sum_{\mu=1}^{P} \delta^\mu_j x^\mu_j
\end{equation}
ostatecznie pełną regułę zapiszemy:
\begin{equation}
       w_{ji}(k+1)  = w_{ji}(k) + \Delta w_{ji},
\end{equation}
Konsekwentna realizacja metody największego spadku wymaga dokonywania zmian wag dopiero po zaprezentowaniu sieci pełnego zbioru wzorców. W praktyce stosuje się jednak zmiany wag po każdej prezentacji wzorca zgodnie ze wzorem:
\begin{equation}
       \Delta ^\mu w_{ji} = -\eta \frac {\partial \xi _\mu}{\partial w_{ji}} = \eta \delta _j ^\mu x^\mu_i,
\end{equation}

\end{easyappendix}



\begin{easyappendix}{ Jednowymiarowa regresja liniowa}
\cite{russell2023} Funkcja liniowa jednej zmiennej to funkcja w postaci \(y=w_1x +w_0\); współczynniki \(w_0\) i \(w_1\) możemy traktować jak wagi, i możemy je traktować łącznie jako wektor \(\textbf{W}=<w_0,w_1>\) a samo przekształcenie można utożsamić z iloczynem skalarnym \(y=\textbf{W}*<1,x> \). Zadanie dopasowania najlepszej hipotezy \(hw\) wiążącej te dwie wielkości nosi nazwę regresji liniowej. Matematycznie dopasowanie to sprowadza się do znalezienia wektora W minimalizującego funkcję straty, zgodnie z teorią Gaussa jako miarę tej straty przyjmuje się sumę miar dla wszystkich przykładów:
\begin{equation*}
       Loss(h_w) = \sum_{j=1}^{N} L_2 \big{(}y_j, hw(x_j)\big{)} = \sum_{j=1}^{N} L_2 \big{(}y_j-hw(x_j)\big{)}^2 =
       \sum_{j=1}^{N} L_2 \big{(}y_j-(w_1x + w_0)\big{)}^2,
\end{equation*}
Naszym celem jest znalezienie optymalnego wektora W 
\begin{equation*}
       \textbf{W} = \text{argmin } Loss(h_w)
\end{equation*}
Gdy funkcja ciągła osiąga minimum w danym punkcie, pierwsze pochodne cząstkowe po argumentach tej funkcji zerują się w tym punkcie; w kontekście regresji liniowej nasza funkcja \(Loss(h_w)\) jest funkcją dwu zmiennych: \(w_0\) i \(w_1\), których wartości w punkcie minimum określone są przez układ równań:
 

    \begin{equation*}
        \Biggl\{
                \begin{matrix}
                    \frac{\partial}{\partial{w_0}} \sum_ \big{(}y_j -(w_1x + w_0)\big{)}^2 = 0,\\
                     
                    \frac{\partial}{\partial{w_1}} \sum \big{(}y_j -(w_1x + w_0)\big{)}^2 = 0,
                \end{matrix} 
    \end{equation*} 

Rozwiązaniem takiego układu są wartości:

\begin{equation}
w_1=\frac{ N(\sum x_jy_j)-(\sum x_j)(\sum y_j) }{ N(x_j^2)-(\sum x_j)^2 }, 
w_0=\frac{\sum y_j - w_1( \sum x_j)}{N},
\end{equation}

Dla dużych N\cite{russell2023} musimy użyć następującej, równoważnej postaci rzeczonych wzorów:
\begin{equation}
w_1=\frac{ \sum (x_j-\overline{x})( y_j - \overline{y}  )  }{ \sum ( x_j - \overline{x} )^2 }, 
w_0=\overline y - w_1\overline x,
\label{eq:linearregresion}
\end{equation}
gdzie \(\overline{x}\) i \(\overline{y} \) są średnimi arytmetycznymi: 
\begin{equation}
\overline{x}=\frac{\sum x_j}{N}, \overline{y}=\frac{\sum y_j}{N},
\end{equation}
\end{easyappendix}

 
