\begin{easyappendix}{Uogólniona reguła delty}
Rozważmy sieć jednowarstwową z elementami przetwarzającymi o nieliniowej, lecz niemalejącej i różniczkowalnej funkcji aktywacji F wówczas zmianę wag przy prezentacji \(\mu\)-tego wzorca  można opisać równaniem:
\begin{equation}
       \Delta w_{ji} = -\eta \frac {\partial \xi}{\partial w_{ji}} = - \eta \frac{\partial \xi}{\partial y_j} \frac{\partial y_j}{\partial w_{ji}} =
       - \eta \frac{\partial \xi}{\partial z_j} 
       \frac{\partial z_j}{\partial y_{j}}
       \frac{\partial y_j}{\partial w_{ji}},
\end{equation}

przy czym:
\begin{equation}
\frac{\partial \xi}{\partial z_j} = (s-z_j), \text{ z def. } ((\frac{1}{2}(x-y)^2 )' = (x-y),
\end{equation}

\begin{equation}
        \frac{\partial z_j}{\partial y_{j}} = F'(y_j),
\end{equation}

\begin{equation}
       \frac{\partial y_j}{\partial w_{ji}} = x_i;
\end{equation}

stąd ostatecznie wzór przyjmuje postać:
\begin{equation}
\Delta w_{ji}= \eta F'(y)(S-z_j)x_i = \eta F'(y)(S-F(y))x_i = \eta F'(y)(s_1, s_2  \dots s_m -F(x_1*w_1,x_2*w_2  \dots ))x_i
\end{equation}
oraz dla warstwy ukrytej:
\begin{equation}
\Delta w_{ji}= \eta F'(y_j) \sum_{i=1}^{n_{m+1}} {F'(y_j)(s^{^{m+1}}-z^{^{m+1}}_j)w_{ji}}
\end{equation}

\end{easyappendix}






\begin{easyappendix}{Algorytm propagacji wstecznej}

Algorytm ten \cite{korbicz1994}, podaje on przepis na zmianę wag \(w_{ij}\) dowolnych połączeń elementów przetwarzających rozmieszczonych w sąsiednich warstwach sieci jednokierunkowej. Jest on oparty na minimalizacji sumy kwadratów błędów uczenia z wykorzystaniem optymalizacyjnej metody największego spadku \cite{wit1986}. Dzięki zastosowaniu specyficznego sposobu propagowania błędów uczenia sieci powstałych na wyjściu, tzn. przesyłania ich do warstwy wyjściowej od wejściowej, algorytm propagacji wstecznej stał się jednym z najskuteczniejszych algorytmów uczenia sieci. 
Rozważamy sieć jednowarstwową o liniowych elementach przetwarzających. Załóżmy, że mamy \(P\)-elementowy zbiór wzorców. Przy prezentacji \(\mu\)-tego wzorca możemy zdefiniować błąd: 
\begin{equation}
       \delta^\mu_j =s^\mu_j-z^\mu_j=s^\mu-y^\mu_j=s^\mu-\sum_{i=0}^{m} w_{ij}x^\mu_i,
\end{equation}
gdzie \(s_j^\mu\), \(y^\mu_j\) oznaczają odpowiednio oczekiwane i aktualne wartości wyjścia \(j\)-tego elementu oraz ważoną sumę wejść wyznaczoną w jego sumatorze przy prezentacji \(\mu\)-tego wzorca. \(x^\mu_i\) i-ta składowa \(\mu\)-tego wektora wejściowego, \(w_{ji}\) - oznacza wagę połączenia pomiędzy \(j\)-tym elementem warstwy wyjściowej a \(i\)-tym elementem warstwy wejściowej. \(m\)-liczba wejść.
\newline
Jako miarę błędu sieci \(\xi\) wprowadzimy sumę po wszystkich wzorcach błędów powstałych przy prezentacji każdego z nich:
\begin{equation}
       \xi = \sum_{\mu=0}^{P}\xi_\mu = \frac {1}{2} \sum ^{P}_{\mu=1} \sum ^{n}_{j=1} (s^\mu-y^\mu)^2,
\end{equation}
gdzie
\begin{equation}
       \xi_\mu = \frac {1}{2} \sum ^{n}_{j=1} (s^\mu-y^\mu)^2,
\end{equation}
\textbf{Problem uczenia sieci to zagadnienie minimalizacji funkcji błędu \(\xi\).} Jedną z najprostszych metod minimalizacji jest gradientowa metoda największego spadku \cite{wit1986}. Jest to metoda iteracyjna, która poszukuje kolejnego lepszego punktu w kierunku przeciwnym do gradientu funkcji celu w danym punkcie. Stosując powyższą metodę do uczenia sieci, zmiana \(\Delta w_{ji}\)  wagi połączenia winna spełniać relację:

\begin{equation}
       \Delta w_{ji} = -\eta \frac{\partial\xi}{\partial w_{ji}} = -\eta \sum ^{P}_{\mu=1} \frac {\partial \xi_\mu}{\partial w_{ji}} = -\eta \sum ^{P}_{\mu=1} \frac {\partial \xi_\mu}{\partial z^\mu_j} \frac{\partial z_j^\mu}{\partial w_{ji}}
\end{equation}
gdzie \(\eta\) oznacza współczynnik proporcjonalności.
W przypadku elementów liniowych mamy:
\begin{equation}
       \frac{\partial\xi_\mu}{\partial z^\mu_j} = -(s^\mu_j-z^\mu_j) = -\delta^\mu_j, 
\end{equation}
\begin{equation}
       \frac{\partial z_j^\mu}{\partial w_{ji}} = \frac{\partial y^\mu_j}{\partial w_{ji} } = x _i^\mu
\end{equation}
stąd otrzymujemy:
\begin{equation}
       \Delta w_{ji} = \eta \sum_{\mu=1}^{P} \delta^\mu_j x^\mu_j
\end{equation}
ostatecznie pełną regułę zapiszemy:
\begin{equation}
       w_{ji}(k+1)  = w_{ji}(k) + \Delta w_{ji},
\end{equation}
Konsekwentna realizacja metody największego spadku wymaga dokonywania zmian wag dopiero po zaprezentowaniu sieci pełnego zbioru wzorców. W praktyce stosuje się jednak zmiany wag po każdej prezentacji wzorca zgodnie ze wzorem:
\begin{equation}
       \Delta ^\mu w_{ji} = -\eta \frac {\partial \xi _\mu}{\partial w_{ji}} = \eta \delta _j ^\mu x^\mu_i,
\end{equation}

\end{easyappendix}


\begin{easyappendix}{Przetwarzanie współbieżne}
 Ponadto przetwarzanie przez sieć z założenia jest \textbf{współbieżne} \cite{ossowski2020}. Natomiast realizacja jest współbieżna tym sensie, że neurony są w obliczeniach od siebie niezależne (korzystają ze wspólnych danych wejściowych bez ich modyfikacji), natomiast wyjście z sieci jest synchronizowane. Ponadto proces przetwarzania w pojedynczym neuronie może być współbieżny \( w_i*x_i \) w części ważenia sygnału wejściowego, natomiast sumowanie sygnałów ważonych jest już synchronizowane.
[!]
\newline  
przyjrzeć się sumowaniu wyników z wątków.
sumowanie wyników w wątkach po 1 parze wyników - jak w drzewie (drzewo sumowania )
\begin{lstlisting}
        = -> sum  - = -> sum - ... 
        = -> sum  -
        ...

               sum
              /    \
             sum   sum
            /   \ /   \ 
            ..... 
\end{lstlisting}    
\end{easyappendix}

\begin{easyappendix}{Dowód nieskończoności urojonej}
\lipsum[8]
\end{easyappendix}
